\documentclass[a4paper, openany]{book}
\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}
\usepackage{graphicx}
\usepackage{amsmath,amssymb, amsfonts, geometry, float, listings, enumerate, multicol}
\usepackage{multicol, float, color, colortbl}
\usepackage{lipsum}
\usepackage{tikz, titlesec, parskip}
\usepackage{tikz,pgfplots, circuitikz}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{spreadtab}
\usetikzlibrary{arrows, decorations.markings}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,calc}
\usepackage{xcolor, soul} 

\pgfplotsset{compat=1.5.1}
\usepgfplotslibrary{fillbetween}
\usepackage{caption, enumitem}
\usepackage{bm}
\usepackage[export]{adjustbox}
\usepackage{mathtools}
\tikzstyle{block} = [draw, fill=white, rectangle, 
    minimum height=3em, minimum width=6em]
    
\tikzstyle{vecArrow} = [thick, decoration={markings,mark=at position
   1 with {\arrow[semithick]{open triangle 90}}},
   double distance=1pt, shorten >= 5.5pt,
   preaction = {decorate},
   postaction = {draw,line width=1pt, white,shorten >= 4.5pt}]
   
\tikzstyle{innerBlue} = [semithick, blue,line width=1pt, shorten >= 4.5pt]

\tikzset{%
    block/.style={draw, fill=white, rectangle, 
            minimum height=2em, minimum width=3em},
    input/.style={inner sep=0pt},       
    output/.style={inner sep=0pt},      
    sum/.style = {draw, fill=white, circle, minimum size=2mm, node distance=1.5cm, inner sep=0pt},
    pinstyle/.style = {pin edge={to-,thin,black}}
}


\usepackage{etoc}
\usepackage{relsize}
\usepackage{systeme}
\usepackage[pagebackref=false,colorlinks,linkcolor=blue,citecolor=magenta]{hyperref}
\usepackage{wrapfig, blindtext}
\titlespacing{\section}{0pt}{10pt}{0pt}
\titlespacing{\subsection}{0pt}{10pt}{0pt}
\titlespacing{\subsubsection}{0pt}{10pt}{0pt}

\usetikzlibrary{calc,patterns,through}
\newcommand{\arcangle}{%
	\mathord{<\mspace{-9mu}\mathrel{)}\mspace{2mu}}%
}


\newcommand{\code}{\texttt} 
\newcommand*{\plogo}{\fbox{$\mathcal{SUT}$}}
\usepackage{listings}



\renewcommand{\baselinestretch}{1.2}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }
 \usepackage{multicol}
\usepackage{color}
\usepackage{transparent}
\setlength{\columnseprule}{1pt}
\def\columnseprulecolor{\color{blue}}

\usepackage{fancyhdr}
\pagestyle{fancy}




\makeatletter
\renewcommand{\thesection}{%
  \ifnum\c@chapter<1 \@arabic\c@section
  \else \thechapter.\@arabic\c@section
  \fi
}
\makeatother


\usepackage{eso-pic}
               \newcommand\BackgroundIm{
               \put(0,0){
               \parbox[b][\paperheight]{\paperwidth}{%
               \vfill
               \centering
               {\transparent{0.3}
               \includegraphics[height=\paperheight,width=\paperwidth,
               keepaspectratio]{images/background.png}%
               }
               \vfill
               }}}


\begin{document}
 \AddToShipoutPicture*{\BackgroundIm}

\begin{titlepage} % Suppresses displaying the page number on the title page and the subsequent page counts as page 1
	
	\raggedleft % Right align the title page
	\rule{1pt}{\textheight} % Vertical line
	\hspace{0.05\textwidth} % Whitespace between the vertical line and title page text
	\parbox[b]{0.75\textwidth}{ % Paragraph box for holding the title page text, adjust the width to move the title page left or right on the page
		
		{\Huge\bfseries DL Course Project}\\[2\baselineskip] % Title
		{\large\textit{Joint Depth Estimation and Object Detection}}\\[4\baselineskip] % Subtitle or further description
		{\Large\textsc{Mohammad Amin Alamalhoda}} \\ % Author name, lower case for consistent small caps
		{\Large\textsc{AmirReza HatamiPour}}\\
		{\Large\textsc{MohammadReza Alimohammadi}}
		
		
		\vspace{0.5\textheight} % Whitespace between the title block and the publisher
		
		{\noindent Sharif University of Technology~~\plogo}\\[\baselineskip] % Publisher and logo
		}

\end{titlepage}

\pagenumbering{roman}



{
  \hypersetup{linkcolor=black}
  \tableofcontents
}

{
  \hypersetup{linkcolor=black}
  \listoffigures
  \listoftables
}


\fancyhf{}

\fancyhead[R]{\includegraphics[width=0.05\textwidth]{Shariflogo.png} }
\fancyhead[L]{MohammadAmin \\ Alamalhoda}
\cfoot{(\space \space \space \space \textbf{\thepage}  \space \space \space)}

\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1pt}


\newpage  
\mainmatter
\pagenumbering{arabic}

\section{Git and Project Dependencies}

\vspace{0.3cm}
\subsection{Git}
	\vspace{0.3cm}

This Project is open source and is published on Github. You can watch it using \href{https://github.com/MohammadAminAlamalhoda/Deep-Object}{this link}.

You can use the following bash command for cloning this project:

\begin{lstlisting}[language=bash]
  $ git clone https://github.com/MohammadAminAlamalhoda/Deep-Object
  \end{lstlisting}
  
If you don't have \code{git} installed on your device, you can use the following bash command:
\begin{itemize}
\item Linux
\begin{lstlisting}[language=bash]
  $ sudo apt-get install git
  \end{lstlisting}
  
\item MacOS

MacOS already have git installed, check its version using bash command below:

\begin{lstlisting}[language=bash]
  $ git --version
  \end{lstlisting}
If you uninstalled it, you can install it using \code{brew}:
\begin{lstlisting}[language=bash]
  $ brew install git
  \end{lstlisting}
\item Windows

You can download source code of git and make\-install it using \href{https://git-scm.com/download/win}{this link}.
\end{itemize}



\subsection{Project Dependencies}
	\vspace{0.3cm}

This project needs the following stuff in order to be compiled successfully.

\begin{itemize}
\item PyTorch - Python Lib
\item Torchvision - Python Lib
\item OpenCV - Python Lib
\item Matplotlib - Python Lib
\item mat73 - Python Lib
\item Os - Python Lib
\item Sys - Python Lib
\item \href{https://pytorch.org/vision/stable/_modules/torchvision/models/detection/mask_rcnn.html}{Mask-RCNN}- Torch Hub Model
\item \href{https://pytorch.org/hub/intelisl_midas_v2/}{MiDaS} - Torch Hub Model


\end{itemize}



\newpage

\section{Project Structure}
	\vspace{0.3cm}
	
This project contains different files. We will explain them in the following.
\subsection{train.ipynb}
	\vspace{0.3cm}
	
This jupyter notebook file contains the configs and essential properties for training the networks.

\subsection{coco\_eval.py and coco\_utils.py}
	\vspace{0.3cm}
This files contain coco tools for image cropping and preparing images for object detection.

\subsection{datas.py}
	\vspace{0.3cm}
This files contains dataloader classes and handles the loading and augmentation of the datas during training.

\subsection{engine.py}
	\vspace{0.3cm}
	
This file contains the functions for training and evaluation of the networks.

\subsection{models.py}
	\vspace{0.3cm}	
	
All the model classes (Object Detector, Depth Estimator, and Concatener) are defined in this file.
	
\subsection{transforms.py}
	\vspace{0.3cm}	
	
Torchvision transfomation for augmenting the data set are defined in this file.
	
\subsection{utils.py}
	\vspace{0.3cm}	
	
This file contain some useful function such as model saver-loader, loggers, and ... .

\subsection{GUI Directory}
	\vspace{0.3cm}	

This directory contains files related to our application for loading and joint object detection and depth estimation of the datas.






	\newpage
	
\section{Datas}
	\vspace{0.3cm}
\subsection{Converting Datas}
	\vspace{0.3cm}
We converted the datas which were in the .mat format to the .png  for better RAM management. This way it is possible to load the images directly from the hard drive in each iteration.

\subsection{Loading Datas}
Figure \ref{fig:sample_datas} shows a sample from the dataset.





\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{images/imgNo14.png}
    \caption{Image No 1}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{images/depthNo14.png}
    \caption{Depth No 1}
  \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{images/labelNo14.png}
    \caption{Label No 1}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{images/imgNo12.png}
    \caption{Image No 2}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{images/depthNo12.png}
    \caption{Depth No 2}
  \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{images/labelNo12.png}
    \caption{Label No 2}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{images/imgNo8.png}
    \caption{Image No 3}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{images/depthNo8.png}
    \caption{Depth No 3}
  \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{images/labelNo8.png}
    \caption{Label No 3}
  \end{subfigure}
   \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{images/imgNo10.png}
    \caption{Image No 4}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{images/depthNo10.png}
    \caption{Depth No 4}
  \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{images/labelNo10.png}
    \caption{Label No 4}
  \end{subfigure}
  \caption{Some Sample Images from The Dataset}
  \label{fig:sample_datas}
\end{figure}

As can be seen in Figure \ref{fig:sample_datas}, dataset contains image, label, and depth.

\newpage

\subsection{Data Augmentation}
	\vspace{0.3cm}
We augmented the datas by resizing to 640, random cropping an 640$\times$640 square, and random horizontal flip. Some of the Augmented Images are plotted in Figure \ref{fig:augment}.



\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{images/imgNo1272.png}
    \caption{Image}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{images/aug_image.png}
    \caption{Augmented Image}
  \end{subfigure}
  \caption{An Image and its Augmented Version}
  \label{fig:augment}
\end{figure}

Transforms are done using \textit{torchvisoin.datas.trasnforms}.
\subsection{Data Loader}
	\vspace{0.3cm}
We used \textit{torch data loaders} for managing datas and augmentation. You can find this files in datas.py file.

\subsection{Loading .mat Dataset}
	\vspace{0.3cm}

Due to RAM shortage, we didn't load the datas on the RAM. All the images, labels and depths in .mat file were converted to .png images and the loaded directly from the hard drive during training. This was done using function \text{mat2png()} which can be find in utils.py


\newpage

	
\section{Models}
	\vspace{0.3cm}
\subsection{Object Detection}
	\vspace{0.3cm}

We used pretrained \textit{Mask-RCNN} as the object detector network and fine tuned it using our dataset. You can find the paper of \textit{Mask-RCNN} using \href{https://arxiv.org/pdf/1703.06870.pdf}{this link}.

\subsubsection{Architecture}

This network develops, for instance segmentation. Instance segmentation is challenging because it requires the correct detection of all objects in an image while precisely segmenting each instance. It therefore combines elements from the classical computer vision tasks of object detection, where the goal is to classify individual objects and localize each using a bounding box, and semantic segmentation, where the goal is to classify each pixel into a fixed set of categories without differentiating object instances.
Mask R-CNN, extends Faster R-CNN by adding a branch for predicting segmentation masks on each Region of Interest (RoI), in parallel with the existing branch for classification and bounding box regression. You can see an example of box regression in the Figure \ref{fig:mask-rcnn}.


\begin{figure}[ht]
  \centering
    \includegraphics[width=0.7\linewidth]{images/mask_rcnn.png}
      \caption{Bounding Box Regression in the Mask-RCNN}
  \label{fig:mask-rcnn}
\end{figure}

The mask branch is a small FCN applied to each RoI, predicting a segmentation mask in a pixel-to-pixel manner. Mask R-CNN is simple to implement and train given the Faster R-CNN framework, which facilitates a wide range of flexible architecture designs. Additionally, the mask branch only adds a small computational overhead, enabling a fast system and rapid experimentation. Faster RCNN was not designed for pixel-to-pixel alignment between network inputs and output. 
To fix the misalignment, proposed a simple, quantization-free layer, called RoIAlign, that faithfully preserves exact spatial locations. Despite being a seemingly minor change, RoIAlign has a large impact: it improves mask accuracy by relative 10\% to 50\%, showing bigger gains under stricter localization metrics. Second, we found it essential to decouple mask and class prediction: we predict a binary mask for each class independently, without competition among classes, and rely on the network’s RoI classification branch to predict the category. 

\subsubsection{Concept}
	\vspace{0.3cm}
Mask R-CNN is conceptually simple: Faster R-CNN has two outputs for each candidate object, a class label, and a bounding-box offset; to this, we add a third branch that outputs the object mask. Mask R-CNN is thus a natural and intuitive idea. But the additional mask output is distinct from the class and box outputs, requiring extraction of an object's much finer spatial layout. Next, we introduce the key elements of Mask R-CNN, including pixel-to-pixel alignment, which is the main missing piece of Fast/Faster R-CNN.
\newpage

\subsubsection{Computation Pricedure}
	\vspace{0.3cm}
	
Mask R-CNN adopts the same two-stage procedure, with an identical first stage (RPN). In parallel to predicting the class and box offset in the second stage, Mask R-CNN also outputs a binary mask for each RoI. This contrasts with most recent systems, where classification depends on mask predictions. Our approach follows the spirit of Fast R-CNN that applies bounding-box classification and regression in parallel (which turned out to largely simplify the multi-stage pipeline of original R-CNN). Formally, during training, we define a multi-task loss on each sampled RoI as:

\begin{eqnarray*}
L &= L_{cs} +L_{box}+L_{mask}
\end{eqnarray*}

The classification loss $L_{cs}$ and bounding-box $L_{bo}$. The mask branch has a $Km^2$-dimensional output for each RoI, which encodes K binary masks of resolution m × m, one for each of the $K$ classes. To this, apply a per-pixel sigmoid and define $L_{mask}$ as the average binary cross-entropy loss, for an RoI associated with ground-truth class $k$, $L_{mask}$ is only defined on the $k^{th}$ mask (other mask outputs do not contribute to the loss).

$L_{mask}$ allows the network to generate masks for every class without competition among classes; They relied on the dedicated classification branch to predict the class label used to select the output mask. This decouples mask and class prediction. This is different from common practice when applying FCNs to semantic segmentation, which typically uses a per-pixel softmax and a multinomial cross-entropy loss. In that case, masks across classes compete; they do not have a per-pixel sigmoid and a binary loss.

You can see an overview of \textit{Mask-RCNN} in the Figure \ref{fig:mask-rcnn-2}

\begin{figure}[ht]
  \centering
    \includegraphics[width=0.7\linewidth]{images/mask-rcnn-2.png}
      \caption{A Schematic of the Mask-RCNN}
  \label{fig:mask-rcnn-2}
\end{figure}

\newpage
\subsubsection{Fine-Tuning}
	\vspace{0.3cm}

As mentioned earlier, we fine tuned the model by using our datas. You can see the training loss plot in Figure \ref{fig:loss_OD}.

\begin{figure}[ht]
  \centering
    \includegraphics[width=1\linewidth]{images/loss_OD.png}
      \caption{Training Loss of the Object Detector Model}
  \label{fig:loss_OD}
\end{figure}


\subsubsection{Evaluation}
	\vspace{0.3cm}







\begin{table}[htp]
\caption{default}
\begin{center}
\begin{tabular}{c | c | c}
Metric & Initial Value & Final Value\\
\hline
Total loss &8.892 & 2.7060\\
Classifier Loss & 3.902 & 1.4148\\
Box Reg Loss& 2.023 & 0.6253\\
Mask Loss & 1.834 & 0.4961\\
Objectness Loss& 0.673 & 0.0435
\end{tabular}
\end{center}
\label{default}
\end{table}%


































\end{document}